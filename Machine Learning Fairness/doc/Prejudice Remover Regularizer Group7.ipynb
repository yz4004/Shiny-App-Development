{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c72bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.layers import Dense, Input\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa4e6ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7214, 10)\n",
      "(7185, 10)\n"
     ]
    }
   ],
   "source": [
    "raw = pd.read_csv(\"./compas-scores-two-years.csv\")\n",
    "\n",
    "# biased feature\n",
    "name_list = [ 'sex','age', 'race', 'juv_fel_count', 'decile_score','juv_misd_count', 'juv_other_count', 'priors_count',\n",
    "             'c_days_from_compas','c_charge_degree', 'c_charge_desc', 'v_decile_score',\n",
    "             'start', 'end', 'event', 'two_year_recid'] # 16\n",
    "\n",
    "# remove decile_score, v_decile_score... those features already contain prejudice\n",
    "name_list1 = [ 'sex','age', 'race', 'priors_count',\n",
    "             'c_charge_degree', 'c_charge_desc',\n",
    "             'start', 'end', 'event', 'two_year_recid'] # 11\n",
    "\n",
    "\n",
    "raw_data_for_train = raw.loc[:,name_list1]\n",
    "\n",
    "print(raw_data_for_train.shape) \n",
    "\n",
    "# dropna 7214 -> 7185\n",
    "raw_data_for_train = raw_data_for_train.dropna()\n",
    "print(raw_data_for_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a6e3122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3684, 10) (2445, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnote\\ntrain_data is a df with dimension (6129 rows × 10 columns) \\nfirst (3684,10) is z/s = 0\\nlast (2445,10) is z/s = 1\\nnow race is still in categorical form\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reorder training data\n",
    "African_American_index = raw_data_for_train.race == 'African-American'\n",
    "Caucasian_index = raw_data_for_train.race == 'Caucasian'\n",
    "print(raw_data_for_train[African_American_index].shape,raw_data_for_train[Caucasian_index].shape)\n",
    "\n",
    "AA_train = raw_data_for_train[African_American_index]\n",
    "CA_train = raw_data_for_train[Caucasian_index]\n",
    "train_data = pd.concat( (AA_train, CA_train) ) \n",
    "\n",
    "'''\n",
    "note\n",
    "train_data is a df with dimension (6129 rows × 10 columns) \n",
    "first (3684,10) is z/s = 0\n",
    "last (2445,10) is z/s = 1\n",
    "now race is still in categorical form\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ff72c",
   "metadata": {},
   "source": [
    "## label and feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367bbb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6129,)\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "11    0\n",
      "13    0\n",
      "15    0\n",
      "17    0\n",
      "20    0\n",
      "21    0\n",
      "27    0\n",
      "Name: race_num, dtype: int64 \n",
      " 7184    1\n",
      "7185    1\n",
      "7187    1\n",
      "7188    1\n",
      "7191    1\n",
      "7192    1\n",
      "7194    1\n",
      "7199    1\n",
      "7205    1\n",
      "7206    1\n",
      "Name: race_num, dtype: int64 0.0 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnote\\nprint(all_features.shape) will return (6129, 423)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## label\n",
    "train_label = train_data.two_year_recid\n",
    "del train_data[\"two_year_recid\"]  \n",
    "print(train_label.shape)\n",
    "\n",
    "## feature\n",
    "# 1. normalizing numerical features\n",
    "numeric_features = train_data.dtypes[train_data.dtypes != 'object'].index\n",
    "train_data[numeric_features] = train_data[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))\n",
    "\n",
    "# 2. convert sensitive feature to numerical feature \n",
    "# race = \"African-American\"/\"Caucasian\"  -->  race_num = 0/1\n",
    "train_data = train_data.assign(race_num = (train_data['race']!= 'African-American') | (train_data['race']== 'Caucasian') ) # warning! + -> |\n",
    "train_data.race_num = train_data.race_num.astype('int64')\n",
    "del train_data[\"race\"]\n",
    "print(train_data.race_num[:10],\"\\n\",train_data.race_num[-10:], train_data.race_num[:3684].mean(),train_data.race_num[3684:].mean())\n",
    "\n",
    "# 3. convert other categorical features to dummy variable (one_hot)\n",
    "all_features = pd.get_dummies(train_data, dummy_na=True) \n",
    "\n",
    "'''\n",
    "note\n",
    "print(all_features.shape) will return (6129, 423)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b384a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "      c_charge_desc_nan  race_num\n",
      "1                     0         0\n",
      "2                     0         0\n",
      "3                     0         0\n",
      "11                    0         0\n",
      "13                    0         0\n",
      "...                 ...       ...\n",
      "7192                  0         1\n",
      "7194                  0         1\n",
      "7199                  0         1\n",
      "7205                  0         1\n",
      "7206                  0         1\n",
      "\n",
      "[6129 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nnote\\n\\n3 print will return\\n423\\nrace_num 422\\n['c_charge_desc_arrest case no charge', 'c_charge_desc_nan', 'race_num'] 423\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Put sensitive variable i.e. race_num to last column\n",
    "reorder_column_name = list(all_features.keys())\n",
    "# print(len(reorder_column_name))\n",
    "race_num_name = reorder_column_name.pop(5)\n",
    "# print(race_num_name,len(reorder_column_name))\n",
    "reorder_column_name.append(race_num_name)\n",
    "# print(reorder_column_name[-3:],len(reorder_column_name))\n",
    "all_features = all_features[reorder_column_name]\n",
    "print(\"test\")\n",
    "print(all_features.iloc[:,-2:])\n",
    "\n",
    "'''\n",
    "note\n",
    "\n",
    "3 print will return\n",
    "423\n",
    "race_num 422\n",
    "['c_charge_desc_arrest case no charge', 'c_charge_desc_nan', 'race_num'] 423\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5394620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4378, 423) (4378,) (876, 423) (876,) (875, 423) (875,)\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Epoch 1/5\n",
      "35/35 [==============================] - 0s 7ms/step - loss: 0.6926 - binary_accuracy: 0.5162 - val_loss: 0.6744 - val_binary_accuracy: 0.6358\n",
      "Epoch 2/5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6559 - binary_accuracy: 0.7037 - val_loss: 0.6413 - val_binary_accuracy: 0.7432\n",
      "Epoch 3/5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.6233 - binary_accuracy: 0.7841 - val_loss: 0.6113 - val_binary_accuracy: 0.8139\n",
      "Epoch 4/5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5941 - binary_accuracy: 0.8234 - val_loss: 0.5841 - val_binary_accuracy: 0.8368\n",
      "Epoch 5/5\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.5680 - binary_accuracy: 0.8442 - val_loss: 0.5594 - val_binary_accuracy: 0.8493\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.5542 - binary_accuracy: 0.8617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5541599988937378, 0.8617143034934998]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create and shuffle train test validation (5:1:1)\n",
    "num_examples = all_features.shape[0]\n",
    "# print(num_examples)\n",
    "indices = list(range(num_examples))\n",
    "random.shuffle(indices)\n",
    "# print(np.max(indices))\n",
    "\n",
    "all_features_shuffled = all_features.values[indices]\n",
    "train_label_shuffled = train_label.values[indices]\n",
    "\n",
    "x_train = all_features_shuffled[:4378]\n",
    "y_train = train_label_shuffled[:4378]\n",
    "\n",
    "x_val = all_features_shuffled[4378:5254]\n",
    "y_val = train_label_shuffled[4378:5254]\n",
    "\n",
    "x_test = all_features_shuffled[5254:]\n",
    "y_test = train_label_shuffled[5254:]\n",
    "\n",
    "#split out white testors\n",
    "x_test_white_ind = x_test[:,-1] == 1\n",
    "x_test_white = x_test[x_test_white_ind]\n",
    "y_test_white = y_test[x_test_white_ind]\n",
    "\n",
    "#black testors\n",
    "x_test_black_ind = x_test[:,-1] == 0\n",
    "x_test_black = x_test[x_test_black_ind]\n",
    "y_test_black = y_test[x_test_black_ind]\n",
    "\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape,y_test.shape)\n",
    "\n",
    "print(\"#\"*100)\n",
    "print(\"#\"*100)\n",
    "########################################\n",
    "########################################\n",
    "\n",
    "## base model \n",
    "def model_1():\n",
    "    feature_input = Input(all_features_shuffled.shape[1],)\n",
    "    y = Dense(2,\"softmax\")(feature_input)\n",
    "    \n",
    "    model = Model(feature_input,y )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(0.001)\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "model_1_1 = model_1()\n",
    "\n",
    "model_1_1.compile(optimizer=adam,loss=loss,metrics=metric,run_eagerly=True)\n",
    "\n",
    "model_1_1.fit(x_train,tf.one_hot(y_train,2),epochs=5,batch_size = 128,validation_data=(x_val,tf.one_hot(y_val,2)))\n",
    "\n",
    "model_1_1.evaluate(x_test,tf.one_hot(y_test,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "410c2eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black\n",
      "False positive rate/ False negative rate  (0.059813084112149535, 0.04672897196261682)\n",
      "white\n",
      "False positive rate/ False negative rate  (0.0058823529411764705, 0.18235294117647058)\n"
     ]
    }
   ],
   "source": [
    "# In test set:\n",
    "\n",
    "def evaluation(model, test_labels, x_test):\n",
    "    y_true = test_labels\n",
    "    y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "    CM = confusion_matrix(y_true, y_pred)/len(test_labels)\n",
    "    # print('False positive rate is: ', CM[0][1], 'False negative rate is: ', CM[1][0])\n",
    "    FPR =  CM[0][1]\n",
    "    FNR =  CM[1][0]\n",
    "    return FPR, FNR\n",
    "\n",
    "#split out white testors\n",
    "x_test_white_ind = x_test[:,-1] == 1\n",
    "x_test_white = x_test[x_test_white_ind]\n",
    "y_test_white = y_test[x_test_white_ind]\n",
    "\n",
    "#black testors\n",
    "x_test_black_ind = x_test[:,-1] == 0\n",
    "x_test_black = x_test[x_test_black_ind]\n",
    "y_test_black = y_test[x_test_black_ind]\n",
    "\n",
    "#black accuracy\n",
    "print(\"black\")\n",
    "print(\"False positive rate/\",\"False negative rate \",evaluation(model_1_1, y_test_black, x_test_black))\n",
    "\n",
    "#white accuracy\n",
    "print(\"white\")\n",
    "print(\"False positive rate/\",\"False negative rate \",evaluation(model_1_1, y_test_white, x_test_white))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd756fc2",
   "metadata": {},
   "source": [
    "False positive means the model falsley make a positive prediction. Criminal will not recidivate, but model think he/she will.\n",
    "\n",
    "False negative means the model falsley make a negative prediction. Criminal will recidivate, but model think he/she will not.\n",
    "\n",
    "FPr of African American criminals is 6.0%, comparing to Caucasian's 0.58%\n",
    "\n",
    "Criminals who are African American, are more likely to be predicted as recidivating in two years.\n",
    "\n",
    "FNr of African American criminals is 4.7%, comparing to Caucasian's 18.23%\n",
    "\n",
    "Criminals who are Caucasian, are much more likely to be predicted as not recidivating in two years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ada9794a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference of FPR between black and white 0.053930731170973065\n",
      "Difference of FNR between black and white -0.13562396921385375\n"
     ]
    }
   ],
   "source": [
    "print(\"Difference of FPR between black and white\" ,\n",
    "      evaluation(model_1_1, y_test_black, x_test_black)[0] - evaluation(model_1_1, y_test_white, x_test_white)[0])\n",
    "print(\"Difference of FNR between black and white\" ,\n",
    "      evaluation(model_1_1, y_test_black, x_test_black)[1] - evaluation(model_1_1, y_test_white, x_test_white)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94195683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In Train set\n",
    "# #white \n",
    "# x_train_white_ind = x_train[:,-1] == 1\n",
    "# x_train_white = x_train[x_train_white_ind]\n",
    "# y_train_white = y_train[x_train_white_ind]\n",
    "\n",
    "# #black \n",
    "# x_train_black_ind = x_train[:,-1] == 0\n",
    "# x_train_black = x_train[x_train_black_ind]\n",
    "# y_train_black = y_train[x_train_black_ind]\n",
    "\n",
    "# #black accuracy\n",
    "# print(\"black\")\n",
    "# print(\"False positive rate/\",\"False negative rate \",evaluation(model_1_1, y_train_black, x_train_black))\n",
    "\n",
    "# #white accuracy\n",
    "# print(\"white\")\n",
    "# print(\"False positive rate/\",\"False negative rate \",evaluation(model_1_1, y_train_white, x_train_white))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef833d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8acb853d",
   "metadata": {},
   "source": [
    "## Prejudice Remover\n",
    "\n",
    "Add a prejudice remover to previous binary cross entropy loss (and also a L2 regularizer)\n",
    "\n",
    "$$ -\\mathcal{L}(D;\\theta) + \\eta \\mathcal{R}( \\mathcal{D}, \\theta) + \\frac{\\lambda}{2} \\| \\theta\\|_{2}^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f91be",
   "metadata": {},
   "source": [
    "$ \\mathcal{M}(Y|X,;\\theta) = h_{\\theta}(x^{T}\\theta)^{y} * (1-h_{\\theta}(x^{T}\\theta))^{1-y} $\n",
    "```\n",
    "model_1_1()\n",
    "```\n",
    "log likelihood:\n",
    "$ L(D;\\theta) = \\sum_{(y_i,x_i,s_i) \\in \\mathcal{D}} \\ln \\mathcal{M}[y_i|x_i,s_i ; \\theta ] $ \n",
    "\n",
    "```\n",
    "BinaryCrossentropy()\n",
    "\n",
    "```\n",
    "\n",
    "$\\mathcal{R}( \\mathcal{D}, \\theta)$\n",
    "\n",
    "$$ PI = \\sum_{X,S} \\tilde{Pr}[X,S] \\sum_{Y} \\mathcal{M}[Y|X,S;\\theta] ln \\frac{\\hat{Pr}[Y,S]} {\\hat{Pr}[S] \\hat{Pr}[Y] } $$\n",
    "\n",
    "which can be rewritten as:\n",
    "\n",
    "\n",
    "$$ PI = \\sum_{(x_i,s_i) \\in \\mathcal{D}} \\sum_{y \\in {\\{0,1\\}}} \\mathcal{M}[Y|x_i,s_i;\\theta] ln \\frac{\\hat{Pr}[y|s_i]} {\\hat{Pr}[y] } $$\n",
    "\n",
    "\n",
    "We approximate $\\hat{Pr}[y|s_i]$ and $\\hat{Pr}[y] $ by sample and model:\n",
    "\n",
    " $$  \\hat{Pr}[y|s = k] = \\frac{ \\sum_{(x_i,s_i = k)} \\mathcal{M}[y|x_i,s_i=k; \\theta]}{| {(x_i,s_i=k) \\in \\mathcal{D}}|} $$\n",
    " \n",
    " $$\\hat{Pr}[y]  = \\frac{ \\sum_{(x_i,s_i)} \\mathcal{M}[y|x_i,s_i; \\theta] }{|\\mathcal{D}|} $$\n",
    " \n",
    " where $ |\\mathcal{D}| $ is size of training set. Here $ s_i \\in {\\{0,1\\}}$ representing African American and Caucasion, i.e. sensitive feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6947595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([1., 1., 1., ..., 1., 1., 1.]),\n",
       " (3684, 423),\n",
       " (2445, 423),\n",
       " (6129, 423))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset for sensitive features for calculate approximate probabilities\n",
    "\n",
    "X_s0 = all_features.values[:3684]\n",
    "X_s1 = all_features.values[3684:]\n",
    "\n",
    "X_full = all_features.values\n",
    "\n",
    "X_s0[:,-1],X_s1[:,-1], X_s0.shape, X_s1.shape,X_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a050475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_1(y_true, y_pred):\n",
    "    \n",
    "    '''\n",
    "    X       n*d\n",
    "    M(X)    n*2\n",
    "    y       n,\n",
    "    \n",
    "    X_s0    n0*d\n",
    "    \n",
    "    X_s1    n1*d\n",
    "    \n",
    "    X_full  (n0+n1)*d\n",
    "    \n",
    "    '''\n",
    "    eta =1\n",
    "    \n",
    "    M = model_1_1\n",
    "\n",
    "    loss_vector = tf.keras.losses.binary_crossentropy( tf.one_hot(y_train,2), M(x_train))\n",
    "\n",
    "    L = tf.reduce_sum(loss_vector)\n",
    "    \n",
    "    Pys0 = tf.reduce_sum( M(X_s0),axis=0 )/X_s0.shape[0]\n",
    "    \n",
    "    Pys1 = tf.reduce_sum( M(X_s1),axis=0 )/X_s1.shape[0]\n",
    "    \n",
    "    Py = tf.reduce_sum( M(X_full),axis=0 )/X_full.shape[0] \n",
    "    \n",
    "    R = tf.reduce_sum(M(X_s0)* tf.math.log(Pys0 /Py))  + tf.reduce_sum(M(X_s1)*tf.math.log(Pys1/Py)) \n",
    "    \n",
    "\n",
    "    loss =  L + eta* R\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "362d2046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "137/137 [==============================] - 5s 18ms/step - loss: 2651.4858 - binary_accuracy: 0.8682 - val_loss: 2391.7537 - val_binary_accuracy: 0.9041\n",
      "Epoch 2/5\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 2204.3621 - binary_accuracy: 0.9022 - val_loss: 2035.1526 - val_binary_accuracy: 0.9167\n",
      "Epoch 3/5\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 1901.8575 - binary_accuracy: 0.9125 - val_loss: 1779.4753 - val_binary_accuracy: 0.9224\n",
      "Epoch 4/5\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 1680.2656 - binary_accuracy: 0.9201 - val_loss: 1588.0308 - val_binary_accuracy: 0.9326\n",
      "Epoch 5/5\n",
      "137/137 [==============================] - 1s 5ms/step - loss: 1511.5671 - binary_accuracy: 0.9244 - val_loss: 1439.7787 - val_binary_accuracy: 0.9349\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 1439.7786 - binary_accuracy: 0.9063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1439.778564453125, 0.9062857031822205]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "def model_1():\n",
    "    feature_input = Input(all_features_shuffled.shape[1],)\n",
    "    y = Dense(2,\"softmax\",kernel_regularizer=regularizers.l2(0.01))(feature_input)\n",
    "    \n",
    "    model = Model(feature_input,y )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(0.001)\n",
    "# loss = keras.losses.BinaryCrossentropy(from_logits=False) \n",
    "loss = custom_loss_1\n",
    "metric = tf.keras.metrics.BinaryAccuracy()\n",
    "\n",
    "model_1_1 = model_1()\n",
    "\n",
    "model_1_1.compile(optimizer=adam,\n",
    "               loss=loss,\n",
    "               metrics=metric \n",
    "               )\n",
    "\n",
    "model_1_1.fit(x_train,tf.one_hot(y_train,2),epochs=5,validation_data=(x_val,tf.one_hot(y_val,2)))\n",
    "\n",
    "model_1_1.evaluate(x_test,tf.one_hot(y_test,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1529c864",
   "metadata": {},
   "source": [
    "Model accuracy on test set improves from 86% to 90% by adding prejudice remover and regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab41a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a66e20db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black\n",
      "False positive rate/ False negative rate  (0.04299065420560748, 0.05794392523364486)\n",
      "white\n",
      "False positive rate/ False negative rate  (0.03823529411764706, 0.04411764705882353)\n"
     ]
    }
   ],
   "source": [
    "# In test set:\n",
    "\n",
    "def evaluation(model, test_labels, x_test):\n",
    "    y_true = test_labels\n",
    "    y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "    CM = confusion_matrix(y_true, y_pred)/len(test_labels)\n",
    "    # print('False positive rate is: ', CM[0][1], 'False negative rate is: ', CM[1][0])\n",
    "    FPR =  CM[0][1]\n",
    "    FNR =  CM[1][0]\n",
    "    return FPR, FNR\n",
    "\n",
    "#white testors\n",
    "x_test_white_ind = x_test[:,-1] == 1\n",
    "x_test_white = x_test[x_test_white_ind]\n",
    "y_test_white = y_test[x_test_white_ind]\n",
    "\n",
    "#black testors\n",
    "x_test_black_ind = x_test[:,-1] == 0\n",
    "x_test_black = x_test[x_test_black_ind]\n",
    "y_test_black = y_test[x_test_black_ind]\n",
    "\n",
    "#black accuracy\n",
    "print(\"black\")\n",
    "print(\"False positive rate/\",\"False negative rate \",evaluation(model_1_1, y_test_black, x_test_black))\n",
    "\n",
    "#white accuracy\n",
    "print(\"white\")\n",
    "print(\"False positive rate/\",\"False negative rate \",evaluation(model_1_1, y_test_white, x_test_white))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d25ce",
   "metadata": {},
   "source": [
    "In test set:\n",
    "\n",
    "FPr for African American decreases 6.0% -> 4.2%. FNr increases 4.7% -> 5.8%. \n",
    "\n",
    "FPr for Caucasion increases 0.58% -> 3.8%. FNr decreases 18.2% -> 4.4%\n",
    "\n",
    "Conclusion: prejudice remover does reduce bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89fc8376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference of FPR between black and white 0.004755360087960417\n",
      "Difference of FNR between black and white 0.013826278174821328\n"
     ]
    }
   ],
   "source": [
    "print(\"Difference of FPR between black and white\" ,\n",
    "      evaluation(model_1_1, y_test_black, x_test_black)[0] - evaluation(model_1_1, y_test_white, x_test_white)[0])\n",
    "print(\"Difference of FNR between black and white\" ,\n",
    "      evaluation(model_1_1, y_test_black, x_test_black)[1] - evaluation(model_1_1, y_test_white, x_test_white)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c150dadd",
   "metadata": {},
   "source": [
    "Comparing to base model without bias remover\n",
    "\n",
    "Difference of FPR between black and white 0.053930731170973065 \\\n",
    "Difference of FNR between black and white -0.13562396921385375"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a77eb",
   "metadata": {},
   "source": [
    "We reduce the absolute difference of FPR and FNR between races. Prejudice remover works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7492f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
